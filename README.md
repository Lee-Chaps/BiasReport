# Bias Audit

## Overview
This project focuses on identifying and analysing potential bias in datasets and model outputs.  
It explores how data-driven systems can produce unequal outcomes and highlights the importance of fairness and transparency in software and AI systems.

The audit was completed as part of an **AI & Machine Learning bootcamp**, applying analytical thinking and ethical considerations to real-world data problems.

---

## What This Project Does
- Examines datasets for patterns of imbalance or unfair representation  
- Analyses outcomes across different groups  
- Uses visualisations to clearly communicate disparities  
- Encourages responsible and ethical use of data in software systems  

---

## Tools & Technologies
- Python  
- Pandas  
- Matplotlib  
- Jupyter Notebook  

---

## Why This Matters
Bias in data and models can lead to unfair or harmful outcomes.  
This project demonstrates how developers can take responsibility by **questioning data, evaluating results, and designing more thoughtful systems**.

---

## Notes
This project reflects a growing interest in combining **software development, data analysis, and ethical thinking** to build better, more responsible technology.
