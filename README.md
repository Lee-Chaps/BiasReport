# Bias Audit

## Overview
This project presents a bias audit of a machine-learning credit scoring model used for loan eligibility decisions. It examines potential bias in both datasets and model outputs, highlighting how data-driven systems can produce unequal outcomes.

The audit evaluates model performance across different credit score groups using quantitative metrics, visual analysis, and comparisons before and after bias-mitigation techniques. It emphasizes fairness, transparency, and accountability in ethical AI, particularly within financial systems.

Completed as part of an AI & Machine Learning bootcamp, this project applies analytical thinking and ethical considerations to real-world data challenges, demonstrating responsible AI practices in software and banking contexts.
---

## What This Project Does
- Examines datasets for patterns of imbalance or unfair representation  
- Analyses outcomes across different groups  
- Uses visualisations to clearly communicate disparities  
- Encourages responsible and ethical use of data in software systems  

---

## Tools & Technologies
- Python  
- Pandas  
- Matplotlib  
- Jupyter Notebook  

---

## Why This Matters
Bias in data and models can lead to unfair or harmful outcomes.  
This project demonstrates how developers can take responsibility by **questioning data, evaluating results, and designing more thoughtful systems**.

---

## Notes
This project reflects a growing interest in combining **software development, data analysis, and ethical thinking** to build better, more responsible technology.
